{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O2JQjjcCvKhN"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CExziYkkFVl5"
      },
      "source": [
        "# **ANYMAL C WITH PROXIMAL POLICY OPTIMIZATION**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0JMdvPhCM8"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMV75RQU7ztN"
      },
      "source": [
        "!pip install mujoco==2.3.1\n",
        "!pip install wandb==0.13.10\n",
        "!pip install mediapy==1.1.4\n",
        "!pip install matplotlib==3.7.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy"
      ],
      "metadata": {
        "id": "wcKEa78qrd6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQyazSWhEtK"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danimatasd/MUJOCO-AIDL.git"
      ],
      "metadata": {
        "id": "J5x_SmQir03r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env MUJOCO_GL=egl"
      ],
      "metadata": {
        "id": "HvScy8A-sqdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import mujoco\n",
        "import wandb\n",
        "\n",
        "from typing import Callable, Optional, Union, List\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
      ],
      "metadata": {
        "id": "muZ9Rj6GsLj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HknLiQ2R7ztP"
      },
      "source": [
        "#Uncomment this and execute if gpu is going to be used in your code\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWlOLyKp7ztR"
      },
      "source": [
        "PROJECT = \"AIDL-PPO-BD\"\n",
        "MUJOCO_STEPS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "EBuMwkdgdAav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IPb1a_T04rP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-6HJSJ7ztS"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, obs_len, act_len):\n",
        "        super(Agent, self).__init__()\n",
        "        \n",
        "        self.obs_len = obs_len\n",
        "        self.act_len = act_len\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(obs_len, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(128,128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,act_len))\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(128,128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        out = self.mlp(state)\n",
        "        action_scores = self.actor(out)\n",
        "        state_value = self.critic(out)\n",
        "        return action_scores, state_value\n",
        "\n",
        "    def compute_action(self, state, action_std):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs, state_value = self(state)\n",
        "        probs = torch.tanh(probs)\n",
        "\n",
        "        action_var = torch.full((self.act_len,), action_std * action_std)\n",
        "        cov_mat = torch.diag(action_var).unsqueeze(dim=0)\n",
        "      \n",
        "        m = torch.distributions.multivariate_normal.MultivariateNormal(probs, cov_mat)\n",
        "        \n",
        "        action = m.sample()\n",
        "\n",
        "        action_clamped = torch.tanh(action)\n",
        "\n",
        "        action_clamped[0][0] = action_clamped[0][0]*0.6-0.1\n",
        "        action_clamped[0][3] = action_clamped[0][3]*0.6+0.1\n",
        "        action_clamped[0][6] = action_clamped[0][6]*0.6-0.1\n",
        "        action_clamped[0][9] = action_clamped[0][9]*0.6+0.1\n",
        "      \n",
        "        return action_clamped.detach().numpy(), m.log_prob(action_clamped).detach().numpy(), state_value.detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2i2gb6upFK"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Env():\n",
        "    def __init__(self):\n",
        "      #Select here the model you want to train: scene.xml or scene3.xml to train with a step on the floor\n",
        "      self.model = mujoco.MjModel.from_xml_path(\"./MUJOCO-AIDL/anybotics_anymal_c/scene.xml\")\n",
        "      self.data = mujoco.MjData(self.model)\n",
        "      self.renderer=mujoco.Renderer(self.model)\n",
        "      mujoco.mj_kinematics(self.model, self.data)\n",
        "      mujoco.mj_forward(self.model, self.data)\n",
        "      self.FRAMERATE = 60 #Hz\n",
        "      self.DURATION = 8 #Seconds\n",
        "      self.TIMESTEP = 0.002 # 0.002 By Default\n",
        "      self.done = False\n",
        "      self.model.opt.timestep = self.TIMESTEP\n",
        "      # Make a new camera, move it to a closer distance.\n",
        "      self.camera = mujoco.MjvCamera()\n",
        "      mujoco.mjv_defaultFreeCamera(self.model, self.camera)\n",
        "      self.camera.distance = 5\n",
        "      self.frames=[]\n",
        "\n",
        "    def reset(self):\n",
        "      mujoco.mj_resetDataKeyframe(self.model, self.data, 0)\n",
        "      self.data.ctrl=np.zeros(12)\n",
        "      state= np.array(self.data.qpos.copy())\n",
        "      state= np.append(state, self.data.qvel.copy())\n",
        "      self.frames.clear()\n",
        "      return state\n",
        "\n",
        "    def step(self, action, render=False):\n",
        "      self.done=False\n",
        "      reward = 0\n",
        "      self.data.ctrl=action\n",
        "      for i in range(MUJOCO_STEPS):\n",
        "        mujoco.mj_step(self.model, self.data)\n",
        "        reward = reward + self.data.qvel[0] + (self.data.qpos[2]-0.5)\n",
        "        if render and (len(self.frames) < self.data.time * self.FRAMERATE):\n",
        "          self.camera.lookat = self.data.body('LH_SHANK').subtree_com\n",
        "          self.renderer.update_scene(self.data, self.camera)\n",
        "          pixels = self.renderer.render()\n",
        "          self.frames.append(pixels.copy())\n",
        "\n",
        "      state= np.array(self.data.qpos.copy())\n",
        "      state= np.append(state, self.data.qvel.copy())\n",
        "      if self.data.time > self.DURATION:\n",
        "        self.done = True\n",
        "      if self.data.qpos[2] < 0.45:\n",
        "        self.done = True\n",
        "        reward = reward - 100\n",
        "      return state, reward, self.done\n",
        "\n",
        "    def close(self,episode,ep_reward):\n",
        "      path=f'./video_{episode}_{ep_reward}.mp4'\n",
        "      media.write_video(path, self.frames, fps=self.FRAMERATE)"
      ],
      "metadata": {
        "id": "aN01GhXMuuzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjk_O0g2__r"
      },
      "source": [
        "# Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AEDlRa27ztT"
      },
      "source": [
        "transition = np.dtype([('s', np.float64, (37,)), ('a', np.float64, (12,)), ('a_logp', np.float64, (12,)),\n",
        "                       ('r', np.float64), ('s_', np.float64, (37,))])\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer_capacity = capacity\n",
        "        self.buffer = np.empty(capacity, dtype=transition)\n",
        "        self.counter = 0\n",
        "\n",
        "    # Stores a transition and returns True or False depending on whether the buffer is full or not\n",
        "    def store(self, transition):\n",
        "        self.buffer[self.counter] = transition\n",
        "        self.counter += 1\n",
        "        if self.counter == self.buffer_capacity:\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Function"
      ],
      "metadata": {
        "id": "3Y-qXBWCZ8ub"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhYRqx977ztT"
      },
      "source": [
        "def train(policy, optimizer, memory, hparams, action_std):\n",
        "\n",
        "    gamma = hparams['gamma']\n",
        "    ppo_epoch = hparams['ppo_epoch']\n",
        "    batch_size = hparams['batch_size']\n",
        "    clip_param = hparams['clip_param']\n",
        "    c1 = hparams['c1']\n",
        "    c2 = hparams['c2']\n",
        "\n",
        "\n",
        "    s = torch.tensor(memory.buffer['s'], dtype=torch.float)\n",
        "    a = torch.tensor(memory.buffer['a'], dtype=torch.float)\n",
        "    r = torch.tensor(memory.buffer['r'], dtype=torch.float).view(-1, 1)\n",
        "    s_ = torch.tensor(memory.buffer['s_'], dtype=torch.float)\n",
        "\n",
        "    old_a_logp = torch.tensor(memory.buffer['a_logp'], dtype=torch.float).view(-1, 1)\n",
        "    action_var = torch.full((12,), action_std * action_std)\n",
        "    cov_mat = torch.diag(action_var).unsqueeze(dim=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_v = r + gamma * policy(s_)[1]\n",
        "        adv = target_v - policy(s)[1]\n",
        "\n",
        "    for _ in range(ppo_epoch):\n",
        "        for index in BatchSampler(SubsetRandomSampler(range(memory.buffer_capacity)), batch_size, False):\n",
        "            probs, _ = policy(s[index])\n",
        "            dist = MultivariateNormal(probs,cov_mat)\n",
        "            entropy = dist.entropy()\n",
        "            \n",
        "            a_logp = dist.log_prob(a[index]).unsqueeze(dim=1)\n",
        "\n",
        "            ratio = torch.exp(a_logp-old_a_logp[index])\n",
        "\n",
        "            surr1 = ratio * adv[index]\n",
        "\n",
        "            surr2 = torch.clamp(ratio,1-clip_param,1+clip_param) * adv[index]\n",
        "\n",
        "            policy_loss = torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.smooth_l1_loss(policy(s[index])[1], target_v[index])\n",
        "            entropy = entropy.mean()\n",
        "\n",
        "            loss = -policy_loss+c1*value_loss-c2*entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return -policy_loss.item(), value_loss.item(), entropy.item(), ratio.mean().item()\n",
        "\n",
        "def test(action_std ,env, policy, episode, render=False):\n",
        "    state, ep_reward, done = env.reset(), 0, False\n",
        "    counter=0\n",
        "    reward_list=[]\n",
        "    cumulative_reward_list=[]\n",
        "    time_list=[]\n",
        "    while not done:\n",
        "        action, _, _ = policy.compute_action(state, action_std)\n",
        "        state, reward, done = env.step(action, render=True)\n",
        "        reward_list.append(reward)\n",
        "        time_list.append(counter*0.002*MUJOCO_STEPS)\n",
        "        ep_reward += reward\n",
        "        cumulative_reward_list.append(ep_reward)\n",
        "        counter = counter + 1\n",
        "\n",
        "    #Saving Video and sending it to wandb\n",
        "    env.close(episode,ep_reward)\n",
        "    wandb.log({\"Video eval\": wandb.Video(f'./video_{episode}_{ep_reward}.mp4', fps=4, format=\"mp4\")})\n",
        "\n",
        "    #Ploting Episode Reward\n",
        "    plt.plot(time_list, reward_list)\n",
        "    plt.plot(time_list, cumulative_reward_list)\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Episode instant and cumulative Reward')\n",
        "\n",
        "    wandb.log({\"Reward eval\": plt})\n",
        "\n",
        "    return ep_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TPjGR9HNNXl"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToEdt20G7ztU"
      },
      "source": [
        "hparams = {\n",
        "      'gamma' : 0.99,\n",
        "      'log_interval' : 50,\n",
        "      'num_episodes': 15000,\n",
        "      'lr' : 1e-5,\n",
        "      'clip_param': 0.1,\n",
        "      'ppo_epoch': 48,\n",
        "      'replay_size': 6400,\n",
        "      'batch_size': 128,\n",
        "      'c1': 1.,\n",
        "      'c2': 0.001,\n",
        "      'std_init': 1.0,\n",
        "      'std_min': 0.6,\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the train or sweep function"
      ],
      "metadata": {
        "id": "vokvFanTaHaA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVl48VR7ztU"
      },
      "source": [
        "def train_or_sweep(is_sweep=True):\n",
        "    \n",
        "    wandb.init(project=PROJECT)        \n",
        "\n",
        "    if is_sweep:\n",
        "      hparams.update(wandb.config)\n",
        "      print('Params updated')\n",
        "      \n",
        "    # Create environment\n",
        "    env = Env()\n",
        "\n",
        "    # Fix random seed (for reproducibility)\n",
        "    seed=0\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # Set number of imputs and actions\n",
        "    n_inputs = 37\n",
        "    n_actions = 12\n",
        "\n",
        "    # Create policy and optimizer\n",
        "    policy = Agent(n_inputs, n_actions)\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=hparams['lr'])\n",
        "    memory = ReplayMemory(hparams['replay_size'])\n",
        "    \n",
        "    # Load here your model and policy to start your train from a saved checkpoint\n",
        "    #policy = torch.load('./volcanic-star-9_12869_Reward-4092.94_policy.pt')\n",
        "    #optimizer = torch.load('./volcanic-star-9_12869_Reward-4092.94_optimizer.pt')\n",
        "\n",
        "    #Initialize the action_std, decay and init for the covariance matrix of the distribution\n",
        "    action_std_decay = -(hparams['std_min']-hparams['std_init'])*hparams['log_interval']/hparams['num_episodes']\n",
        "    action_std_init = hparams['std_init']\n",
        "    action_std = action_std_init\n",
        "\n",
        "    #Define the target_reward to stop the run before reaching the num_episodes value\n",
        "    target_reward=10000\n",
        "    print(f\"Target reward: {target_reward}\")\n",
        "\n",
        "    # Training loop\n",
        "    running_reward = -100\n",
        "    saving_reward = 0\n",
        "    for i_episode in range(hparams['num_episodes']):\n",
        "        # Collect experience\n",
        "        state, ep_reward, done = env.reset(), 0, False\n",
        "\n",
        "        while not done:  # Don't infinite loop while learning\n",
        "            action, a_logp, state_value = policy.compute_action(state, action_std)\n",
        "            next_state, reward, done = env.step(action, render=False)\n",
        "\n",
        "            if memory.store((state, action, a_logp, reward, next_state)):\n",
        "                policy_loss, value_loss, avg_entropy, ratio = train(policy, optimizer, memory, hparams, action_std)\n",
        "                wandb.log(\n",
        "                    {\n",
        "                    'policy_loss': policy_loss,\n",
        "                    'value_loss': value_loss,\n",
        "                    'avg_reward': running_reward,\n",
        "                    'avg_entropy': avg_entropy,\n",
        "                    'ratio': ratio\n",
        "                    })\n",
        "\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward\n",
        "        running_reward = round(0.05 * ep_reward + (1 - 0.05) * running_reward,2)\n",
        "        \n",
        "        #Log to check episode rewards and std\n",
        "        if i_episode % hparams['log_interval'] == 0:\n",
        "            print(f'Episode {i_episode}\\tLast reward: {ep_reward:.2f}\\tAverage reward: {running_reward:.2f}\\tAction standard deviation: {action_std:.5f}')\n",
        "            action_std = action_std - action_std_decay\n",
        "            action_std = round(action_std, 5)\n",
        "        \n",
        "        #Define the minimum reward needed to save a model and make a video\n",
        "        if running_reward > 2000:\n",
        "            if running_reward > saving_reward:\n",
        "                saving_reward = running_reward\n",
        "                torch.save(policy, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_policy.pt')\n",
        "                torch.save(optimizer, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_optimizer.pt')\n",
        "                wandb.save(f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_policy.pt')\n",
        "                wandb.save(f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_optimizer.pt')\n",
        "                print(f'Policy and Optimizer have been saved')\n",
        "                ep_reward = test(action_std, env, policy,i_episode)\n",
        "                print(f'Episode {i_episode} Video reward: {ep_reward}')\n",
        "\n",
        "        if running_reward > target_reward:\n",
        "            print(\"Solved!\")\n",
        "            torch.save(policy, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_policy.pt')\n",
        "            torch.save(optimizer, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_optimizer.pt')\n",
        "            wandb.save(f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_policy.pt')\n",
        "            wandb.save(f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_optimizer.pt')\n",
        "            ep_reward = test(action_std, env, policy,i_episode)\n",
        "            print(f'Episode {i_episode} Video reward: {ep_reward}')\n",
        "\n",
        "            break\n",
        "\n",
        "    print(f\"Finished training! Running reward is now {running_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execute this to train with the specified hparams"
      ],
      "metadata": {
        "id": "WM2kpz7swLgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_or_sweep(is_sweep=False)"
      ],
      "metadata": {
        "id": "kRWltf33wHd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter sweeping\n",
        "Here you can run a sweep for Hyperparameters and fine tune the longer training.\n",
        "\n",
        "A Higher number of episodes will help do better sweeps but it will take considerably longer to do."
      ],
      "metadata": {
        "id": "O2JQjjcCvKhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "    \"name\": f\"ppo_sweep_0\",\n",
        "    \"method\": 'bayes',\n",
        "    \"metric\": {\n",
        "        \"name\": \"avg_reward\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"lr\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 0.0001,\n",
        "          \"min\": 0.00001\n",
        "        },\n",
        "        \"ppo_epoch\": {\n",
        "          \"distribution\": \"int_uniform\",\n",
        "          \"max\": 60,\n",
        "          \"min\": 40\n",
        "        },\n",
        "        \"c2\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 0.01,\n",
        "          \"min\": 0.001\n",
        "        },\n",
        "        \"replay_size\": {\n",
        "          \"distribution\": \"int_uniform\",\n",
        "          \"max\": 10000.,\n",
        "          \"min\": 6000.\n",
        "        },\n",
        "        \"std_init\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 1.1,\n",
        "          \"min\": 1.0\n",
        "        },\n",
        "        \"std_min\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 0.8,\n",
        "          \"min\": 0.7\n",
        "        }\n",
        "    }\n",
        "  }"
      ],
      "metadata": {
        "id": "I5HEquW7WiBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project=PROJECT)\n",
        "wandb.agent(sweep_id, function=train_or_sweep, count=50, project=PROJECT)"
      ],
      "metadata": {
        "id": "jzSEphzeWo6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}