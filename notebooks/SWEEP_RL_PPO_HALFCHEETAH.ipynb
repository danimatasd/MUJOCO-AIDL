{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8X0JMdvPhCM8",
        "PzQyazSWhEtK",
        "7IPb1a_T04rP",
        "LFjk_O0g2__r",
        "3Y-qXBWCZ8ub",
        "0TPjGR9HNNXl",
        "xEVmzTJjZ_Xg",
        "vokvFanTaHaA",
        "O2JQjjcCvKhN"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CExziYkkFVl5"
      },
      "source": [
        "# **HALF CHEETAH WITH PROXIMAL POLICY OPTIMIZATION**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0JMdvPhCM8"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMV75RQU7ztN"
      },
      "source": [
        "!pip install swig --quiet \n",
        "!pip install wandb --quiet \n",
        "!pip install install free-mujoco-py --quiet \n",
        "!pip install pyvirtualdisplay==0.2.* --quiet \n",
        "!pip install PyOpenGL==3.1.* --quiet \n",
        "!pip install PyOpenGL-accelerate==3.1.* --quiet \n",
        "!pip install jedi --quiet \n",
        "!pip install imageio --quiet \n",
        "!pip install pyglet==1.4 --quiet \n",
        "!pip install stable-baselines3[extra] --quiet  \n",
        "                      "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "!apt-get install -y patchelf    \n",
        "!apt-get -qq install -y xvfb x11-utils\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb"
      ],
      "metadata": {
        "id": "keCb1-KnZiFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQyazSWhEtK"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpAZq1Pq7ztO"
      },
      "source": [
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import timeit\n",
        "import warnings\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymf9L79g7ztP"
      },
      "source": [
        "import gym\n",
        "import wandb\n",
        "import random\n",
        "import mujoco_py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "\n",
        "from random import randint\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HknLiQ2R7ztP"
      },
      "source": [
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ9v0ENW7ztQ"
      },
      "source": [
        "# starting a fake screen in the background\n",
        "#  in order to render videos\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1024,768),color_depth=24)\n",
        "virtual_display.start()\n",
        "\n",
        "\n",
        "# utility to get video file from directory\n",
        "def get_video_filename(dir=\"video\"):\n",
        "  glob_mp4 = os.path.join(dir, \"*.mp4\") \n",
        "  mp4list = glob.glob(glob_mp4)\n",
        "  assert len(mp4list) > 0, \"couldnt find video files\"\n",
        "  return mp4list[-1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWlOLyKp7ztR"
      },
      "source": [
        "PROJECT = \"AIDL-PPO-HALFCHEETAH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "EBuMwkdgdAav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IPb1a_T04rP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-6HJSJ7ztS"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, obs_len, act_len, action_std_init):\n",
        "        super(Agent, self).__init__()\n",
        "        \n",
        "        self.obs_len = obs_len\n",
        "        self.act_len = act_len\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(obs_len, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Linear(128, act_len)\n",
        "        self.critic = nn.Linear(128, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, state):\n",
        "        out = self.mlp(state)\n",
        "        action_scores = self.actor(out)\n",
        "        state_value = self.critic(out)\n",
        "        return torch.tanh(action_scores), state_value\n",
        "\n",
        "    def compute_action(self, state, action_std):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs, state_value = self(state)\n",
        "\n",
        "        action_var = torch.full((self.act_len,), action_std * action_std)\n",
        "        cov_mat = torch.diag(action_var).unsqueeze(dim=0)\n",
        "      \n",
        "        m = torch.distributions.multivariate_normal.MultivariateNormal(probs, cov_mat)\n",
        "        \n",
        "        action = m.sample()\n",
        "        \n",
        "        action_clamped = action.clamp(-1.0, 1.0)\n",
        "      \n",
        "        return action_clamped.detach().numpy(), m.log_prob(action_clamped).detach().numpy(), state_value.detach()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjk_O0g2__r"
      },
      "source": [
        "# Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AEDlRa27ztT"
      },
      "source": [
        "transition = np.dtype([('s', np.float64, (17,)), ('a', np.float64, (6,)), ('a_logp', np.float64, (6,)),\n",
        "                      ('r', np.float64), ('s_', np.float64, (17,))])\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer_capacity = capacity\n",
        "        self.buffer = np.empty(capacity, dtype=transition)\n",
        "        self.counter = 0\n",
        "\n",
        "    # Stores a transition and returns True or False depending on whether the buffer is full or not\n",
        "    def store(self, transition):\n",
        "        self.buffer[self.counter] = transition\n",
        "        self.counter += 1\n",
        "        if self.counter == self.buffer_capacity:\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Function"
      ],
      "metadata": {
        "id": "3Y-qXBWCZ8ub"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhYRqx977ztT"
      },
      "source": [
        "from gym.wrappers.rescale_action import RescaleAction\n",
        "\n",
        "def train(policy, optimizer, memory, hparams, action_std):\n",
        "\n",
        "    gamma = hparams['gamma']\n",
        "    ppo_epoch = hparams['ppo_epoch']\n",
        "    batch_size = hparams['batch_size']\n",
        "    clip_param = hparams['clip_param']\n",
        "    c1 = hparams['c1']\n",
        "    c2 = hparams['c2']\n",
        "\n",
        "\n",
        "    s = torch.tensor(memory.buffer['s'], dtype=torch.float)\n",
        "    a = torch.tensor(memory.buffer['a'], dtype=torch.float)\n",
        "    r = torch.tensor(memory.buffer['r'], dtype=torch.float).view(-1, 1)\n",
        "    s_ = torch.tensor(memory.buffer['s_'], dtype=torch.float)\n",
        "\n",
        "    old_a_logp = torch.tensor(memory.buffer['a_logp'], dtype=torch.float).view(-1, 1)\n",
        "    action_var = torch.full((6,), action_std*action_std)\n",
        "    cov_mat = torch.diag(action_var).unsqueeze(dim=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_v = r + gamma * policy(s_)[1]\n",
        "        adv = target_v - policy(s)[1]\n",
        "\n",
        "    for _ in range(ppo_epoch):\n",
        "        for index in BatchSampler(SubsetRandomSampler(range(memory.buffer_capacity)), batch_size, False):\n",
        "            probs, _ = policy(s[index])\n",
        "            dist = MultivariateNormal(probs, cov_mat)\n",
        "            entropy = dist.entropy()\n",
        "            \n",
        "            a_logp = dist.log_prob(a[index]).unsqueeze(dim=1)\n",
        "\n",
        "\n",
        "            ratio = torch.exp(a_logp-old_a_logp[index])\n",
        "\n",
        "            surr1 = ratio * adv[index]\n",
        "\n",
        "            surr2 = torch.clamp(ratio,1-clip_param,1+clip_param) * adv[index]\n",
        "\n",
        "            policy_loss = torch.min(surr1, surr2).mean()\n",
        "            value_loss = F.smooth_l1_loss(policy(s[index])[1], target_v[index])\n",
        "            entropy = entropy.mean()\n",
        "\n",
        "            loss = -policy_loss+c1*value_loss-c2*entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "    return -policy_loss.item(), value_loss.item(), entropy.item(), ratio.mean().item()\n",
        "\n",
        "#The test function does a episode on the enviroment of the model and renders and saves a video in Wandb\n",
        "\n",
        "def test( action_std, env, policy, render=False):\n",
        "    state, ep_reward, done = env.reset(), 0, False\n",
        "    while not done:\n",
        "        action, _, _ = policy.compute_action(state, action_std)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "    env.close()\n",
        "    mp4 = get_video_filename()\n",
        "    wandb.log({\"Video eval\": wandb.Video(mp4, fps=4, format=\"mp4\")})\n",
        "    return ep_reward"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TPjGR9HNNXl"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToEdt20G7ztU"
      },
      "source": [
        "hparams = {\n",
        "    'gamma' : 0.99,\n",
        "    'log_interval' : 10,\n",
        "    'num_episodes': 50000,\n",
        "    'lr' : 1e-4,\n",
        "    'clip_param': 0.1,\n",
        "    'ppo_epoch': 45,\n",
        "    'replay_size': 600,\n",
        "    'batch_size': 128,\n",
        "    'c1': 3.,\n",
        "    'c2': 0.01,\n",
        "    'std_init': 1.0,\n",
        "    'video_interval': 200\n",
        "}\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gym environment"
      ],
      "metadata": {
        "id": "xEVmzTJjZ_Xg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwjyGoO7ztU"
      },
      "source": [
        "# Create environment\n",
        "env_name = \"HalfCheetah-v3\"\n",
        "env = gym.make(env_name)\n",
        "\n",
        "#Delete the comment of the wrapper clause so you can save videos of the enviroment.\n",
        "#Take in account that rendering videos makes the training longer.\n",
        "\n",
        "#env = gym.wrappers.RecordVideo(env, \"./video\" )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip16yT9u7ztU"
      },
      "source": [
        "# Get number of actions from gym action space\n",
        "n_inputs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "print(n_inputs)\n",
        "print(n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running the code in the previous environment with the prepared hyperparameters"
      ],
      "metadata": {
        "id": "vokvFanTaHaA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVl48VR7ztU"
      },
      "source": [
        "# Fix random seed (for reproducibility)\n",
        "seed=0\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Initialize wandb run\n",
        "wandb.finish() # execute to avoid overlapping runnings (advice: later remove duplicates in wandb)\n",
        "wandb.init(project=PROJECT, config=hparams)\n",
        "wandb.run.name = 'ppo_halfcheetah_train_proper'\n",
        "\n",
        "#Initialize the action_std, decay and init for the covariance matrix of the distribution\n",
        "action_std_decay = ((hparams['log_interval']/hparams['num_episodes'])*hparams['std_init'])\n",
        "action_std_init = hparams['std_init'] + (action_std_decay) #We add a decay before hand because on the episode 0 it will decay with the log of that episode.\n",
        "action_std = action_std_init\n",
        "\n",
        "# Create policy and optimizer\n",
        "policy = Agent(n_inputs, n_actions, action_std_init)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=hparams['lr'])\n",
        "\n",
        "memory = ReplayMemory(hparams['replay_size'])\n",
        "\n",
        "# Training loop\n",
        "print(\"Target reward: {}\".format(env.spec.reward_threshold))\n",
        "running_reward = -100\n",
        "ep_rew_history_reinforce = []\n",
        "for i_episode in range(hparams['num_episodes']):\n",
        "    # Collect experience\n",
        "   \n",
        "    state = env.reset() \n",
        "    ep_reward, done  = 0, False\n",
        "\n",
        "    while not done:  # Don't infinite loop while learning\n",
        "        action, a_logp, state_value = policy.compute_action(state, action_std)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if memory.store((state, action, a_logp, reward, next_state)):\n",
        "            policy_loss, value_loss, avg_entropy, ratio = train(policy, optimizer, memory, hparams, action_std)\n",
        "            wandb.log(\n",
        "                {\n",
        "                'policy_loss': policy_loss,\n",
        "                'value_loss': value_loss,\n",
        "                'running_reward': running_reward,\n",
        "                'mean_entropy': avg_entropy,\n",
        "                'ratio': ratio\n",
        "                })\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward\n",
        "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    \n",
        "    \n",
        "    ep_rew_history_reinforce.append((i_episode, ep_reward))\n",
        "    if i_episode % hparams['log_interval'] == 0:\n",
        "        print(f'Episode {i_episode}\\tLast reward: {ep_reward:.2f}\\tAverage reward: {running_reward:.2f}')\n",
        "        action_std = action_std - action_std_decay\n",
        "        action_std = round(action_std, 5)\n",
        "        #Empirical limit we found of the action_std where below 0.22 it will drop the entropy to a level where the agent stops working\n",
        "        if action_std < 0.250:\n",
        "          action_std = 0.250\n",
        "        print(f'Action_std {action_std}')\n",
        "\n",
        "    if running_reward > 3000.0:\n",
        "        if running_reward > saving_reward:\n",
        "            saving_reward = running_reward\n",
        "            torch.save(policy, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_policy.pt')\n",
        "            torch.save(optimizer, f'./{wandb.run.name}_{i_episode}_Reward-{running_reward}_optimizer.pt')\n",
        "            print(f'Policy and Optimizer have been saved')\n",
        "\n",
        "#Delete the comment of the if clause so you can save videos with the hyperparameter video_interval. \n",
        "#Take in account that rendering videos makes the training longer.\n",
        "\n",
        "    #if i_episode % hparams['video_interval'] == 0:\n",
        "        #ep_reward = test(action_std,test_env, policy)  \n",
        "\n",
        "    if running_reward > env.spec.reward_threshold:\n",
        "        print(\"Solved!\")\n",
        "        break\n",
        "\n",
        "print(f\"Finished training! Running reward is now {running_reward}\")\n",
        "ep_reward = test(action_std,env, policy)  \n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter sweeping\n",
        "Here you can run a sweep for Hyperparameters and fine tune the longer training.\n",
        "\n",
        "A Higher number of episodes will help do better sweeps but it will take considerably longer to do."
      ],
      "metadata": {
        "id": "O2JQjjcCvKhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sweep():\n",
        "    hparams = {\n",
        "    'gamma' : 0.99,\n",
        "    'log_interval' : 20,\n",
        "    'num_episodes': 2000,\n",
        "    'lr' : 1e-4,\n",
        "    'clip_param': 0.1,\n",
        "    'ppo_epoch': 4,\n",
        "    'replay_size': 500,\n",
        "    'batch_size': 128,\n",
        "    'c1': 3.,\n",
        "    'c2': 0.01,\n",
        "    'std_init': 1.0\n",
        "    }\n",
        "\n",
        "    run = wandb.init(PROJECT)\n",
        "    hparams.update(wandb.config)\n",
        "    \n",
        "    # Create environment\n",
        "    env_name = \"HalfCheetah-v3\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # Fix random seed (for reproducibility)\n",
        "    seed=0\n",
        "    env.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # Get number of actions from gym action space\n",
        "    n_inputs = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.shape[0]\n",
        "\n",
        "    #Initialize the action_std, decay and init for the covariance matrix of the distribution    \n",
        "    action_std_decay = (hparams['log_interval']/hparams['num_episodes'])*hparams['std_init']\n",
        "    action_std_init = hparams['std_init']\n",
        "    action_std = action_std_init\n",
        "\n",
        "    # Create policy and optimizer\n",
        "    policy = Agent(n_inputs, n_actions, 1.0)\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=hparams['lr'])\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    memory = ReplayMemory(hparams['replay_size'])\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Target reward: {}\".format(env.spec.reward_threshold))\n",
        "    running_reward = -100\n",
        "    ep_rew_history_reinforce = []\n",
        "    for i_episode in range(hparams['num_episodes']):\n",
        "        # Collect experience\n",
        "        state = env.reset()\n",
        "        ep_reward, done =  0, False\n",
        "        while not done:  # Don't infinite loop while learning\n",
        "            action, a_logp, state_value = policy.compute_action(state, action_std)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if memory.store((state, action, a_logp, reward, next_state)):\n",
        "                policy_loss, value_loss, avg_entropy, ratio = train(policy, optimizer, memory, hparams,action_std)\n",
        "                wandb.log(\n",
        "                    {\n",
        "                    'policy_loss': policy_loss,\n",
        "                    'value_loss': value_loss,\n",
        "                    'avg_reward': running_reward,\n",
        "                    'avg_entropy': avg_entropy,\n",
        "                    'ratio': ratio\n",
        "                    })\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            ep_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        \n",
        "        \n",
        "        ep_rew_history_reinforce.append((i_episode, ep_reward))\n",
        "\n",
        "        #Every log interval we also decay the action std to reduce the entropy and eventually converge on a solution.\n",
        "\n",
        "        if i_episode % hparams['log_interval'] == 0:\n",
        "            print(f'Episode {i_episode}\\tLast reward: {ep_reward:.2f}\\tAverage reward: {running_reward:.2f}') \n",
        "            action_std = action_std - action_std_decay\n",
        "            action_std = round(action_std, 5)\n",
        "            print(f'Action_std {action_std}')\n",
        "\n",
        "    print(f\"Finished training! Running reward is now {running_reward}\")"
      ],
      "metadata": {
        "id": "9BQ0Bib-WRLS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": f\"ppo_sweep_0\",\n",
        "    \"method\": 'bayes',\n",
        "    \"metric\": {\n",
        "        \"name\": \"avg_reward\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"lr\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 0.001,\n",
        "          \"min\": 0.000001\n",
        "        },\n",
        "        \"ppo_epoch\": {\n",
        "          \"distribution\": \"int_uniform\",\n",
        "          \"max\": 100,\n",
        "          \"min\": 2\n",
        "        },\n",
        "        \"c1\": {\n",
        "          \"distribution\": \"int_uniform\",\n",
        "          \"max\": 3.,\n",
        "          \"min\": 1.\n",
        "        },\n",
        "        \"c2\": {\n",
        "          \"distribution\": \"uniform\",\n",
        "          \"max\": 0.1,\n",
        "          \"min\": 0.005\n",
        "        },\n",
        "        \"replay_size\": {\n",
        "          \"distribution\": \"int_uniform\",\n",
        "          \"max\": 1000.,\n",
        "          \"min\": 256.\n",
        "        }\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "I5HEquW7WiBc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "wandb.agent(sweep_id, function=train_sweep, count=30, project=PROJECT)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "jzSEphzeWo6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}