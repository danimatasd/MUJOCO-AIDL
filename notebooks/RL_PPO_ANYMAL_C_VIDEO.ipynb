{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8X0JMdvPhCM8",
        "PzQyazSWhEtK",
        "7IPb1a_T04rP",
        "LFjk_O0g2__r",
        "3Y-qXBWCZ8ub",
        "0TPjGR9HNNXl",
        "xEVmzTJjZ_Xg",
        "vokvFanTaHaA",
        "O2JQjjcCvKhN"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEQCxcYE5bZE"
      },
      "source": [
        "##ANYMAL_C Video output from a run of a previously trained model from our other Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0JMdvPhCM8"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMV75RQU7ztN"
      },
      "source": [
        "!pip install mujoco==2.3.1\n",
        "!pip install mediapy==1.1.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy"
      ],
      "metadata": {
        "id": "wcKEa78qrd6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQyazSWhEtK"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danimatasd/MUJOCO-AIDL.git"
      ],
      "metadata": {
        "id": "J5x_SmQir03r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env MUJOCO_GL=egl"
      ],
      "metadata": {
        "id": "HvScy8A-sqdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import mujoco\n",
        "\n",
        "from typing import Callable, Optional, Union, List\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "muZ9Rj6GsLj_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWlOLyKp7ztR"
      },
      "source": [
        "MUJOCO_STEPS = 5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IPb1a_T04rP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-6HJSJ7ztS"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, obs_len, act_len):\n",
        "        super(Agent, self).__init__()\n",
        "        \n",
        "        self.obs_len = obs_len\n",
        "        self.act_len = act_len\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(obs_len, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(128,128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,act_len))\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(128,128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        out = self.mlp(state)\n",
        "        action_scores = self.actor(out)\n",
        "        state_value = self.critic(out)\n",
        "        return action_scores, state_value\n",
        "\n",
        "    def compute_action(self, state, action_std):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs, state_value = self(state)\n",
        "        probs = torch.tanh(probs)\n",
        "\n",
        "        action_var = torch.full((self.act_len,), action_std * action_std)\n",
        "        cov_mat = torch.diag(action_var).unsqueeze(dim=0)\n",
        "      \n",
        "        m = torch.distributions.multivariate_normal.MultivariateNormal(probs, cov_mat)\n",
        "        \n",
        "        action = m.sample()\n",
        "\n",
        "        action_clamped = torch.tanh(action)\n",
        "\n",
        "        action_clamped[0][0] = action_clamped[0][0]*0.6-0.1\n",
        "        action_clamped[0][3] = action_clamped[0][3]*0.6+0.1\n",
        "        action_clamped[0][6] = action_clamped[0][6]*0.6-0.1\n",
        "        action_clamped[0][9] = action_clamped[0][9]*0.6+0.1\n",
        "      \n",
        "        return action_clamped.detach().numpy(), m.log_prob(action_clamped).detach().numpy(), state_value.detach()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2i2gb6upFK"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Env():\n",
        "    def __init__(self):\n",
        "      #Select here the model you want to train: scene.xml or scene3.xml to train with a step on the floor\n",
        "      self.model = mujoco.MjModel.from_xml_path(\"./MUJOCO-AIDL/anybotics_anymal_c/scene.xml\")\n",
        "      self.data = mujoco.MjData(self.model)\n",
        "      self.renderer=mujoco.Renderer(self.model)\n",
        "      mujoco.mj_kinematics(self.model, self.data)\n",
        "      mujoco.mj_forward(self.model, self.data)\n",
        "      self.FRAMERATE = 60 #Hz\n",
        "      self.DURATION = 8 #Seconds\n",
        "      self.TIMESTEP = 0.002 # 0.002 By Default\n",
        "      self.done = False\n",
        "      self.model.opt.timestep = self.TIMESTEP\n",
        "      # Make a new camera, move it to a closer distance.\n",
        "      self.camera = mujoco.MjvCamera()\n",
        "      mujoco.mjv_defaultFreeCamera(self.model, self.camera)\n",
        "      self.camera.distance = 5\n",
        "      self.frames=[]\n",
        "\n",
        "    def reset(self):\n",
        "      mujoco.mj_resetDataKeyframe(self.model, self.data, 0)\n",
        "      self.data.ctrl=np.zeros(12)\n",
        "      state= np.array(self.data.qpos.copy())\n",
        "      state= np.append(state, self.data.qvel.copy())\n",
        "      self.frames.clear()\n",
        "      return state\n",
        "\n",
        "    def step(self, action, render=False):\n",
        "      self.done=False\n",
        "      reward = 0\n",
        "      self.data.ctrl=action\n",
        "      for i in range(MUJOCO_STEPS):\n",
        "        mujoco.mj_step(self.model, self.data)\n",
        "        reward = reward + self.data.qvel[0] + (self.data.qpos[2]-0.5)\n",
        "        if render and (len(self.frames) < self.data.time * self.FRAMERATE):\n",
        "          self.camera.lookat = self.data.body('LH_SHANK').subtree_com\n",
        "          self.renderer.update_scene(self.data, self.camera)\n",
        "          pixels = self.renderer.render()\n",
        "          self.frames.append(pixels.copy())\n",
        "\n",
        "      state= np.array(self.data.qpos.copy())\n",
        "      state= np.append(state, self.data.qvel.copy())\n",
        "      if self.data.time > self.DURATION:\n",
        "        self.done = True\n",
        "      if self.data.qpos[2] < 0.45:\n",
        "        self.done = True\n",
        "        reward = reward - 100\n",
        "      return state, reward, self.done\n",
        "\n",
        "    def close(self,episode,ep_reward):\n",
        "      path=f'./video_{episode}_{ep_reward}.mp4'\n",
        "      media.write_video(path, self.frames, fps=self.FRAMERATE)"
      ],
      "metadata": {
        "id": "aN01GhXMuuzW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Test Function"
      ],
      "metadata": {
        "id": "3Y-qXBWCZ8ub"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhYRqx977ztT"
      },
      "source": [
        "def test(action_std ,env, policy, episode, render=False):\n",
        "    state, ep_reward, done = env.reset(), 0, False\n",
        "    counter=0\n",
        "    reward_list=[]\n",
        "    time_list=[]\n",
        "    while not done:\n",
        "        action, _, _ = policy.compute_action(state, action_std)\n",
        "        state, reward, done = env.step(action, render=True)\n",
        "        reward_list.append(reward)\n",
        "        time_list.append(counter*0.002*MUJOCO_STEPS)\n",
        "        ep_reward += reward\n",
        "        counter = counter + 1\n",
        "\n",
        "    #Saving Video and sending it to wandb\n",
        "    env.close(episode,ep_reward)\n",
        "\n",
        "    return ep_reward"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the make video function"
      ],
      "metadata": {
        "id": "vokvFanTaHaA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVl48VR7ztU"
      },
      "source": [
        "def make_video(num_videos=10, std_init=0.85820):\n",
        "  \n",
        "  env = Env()\n",
        "  \n",
        "  # We should use the same seed as in the training but it shouldnt affect too much with the pretrained model.\n",
        "  seed=0\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "  # Set number of actions from gym action space\n",
        "  n_inputs = 37\n",
        "  n_actions = 12\n",
        "\n",
        "  # Create policy\n",
        "  policy = Agent(n_inputs, n_actions)\n",
        "\n",
        "  # Be sure to load properly your model here  \n",
        "  policy = torch.load('./MUJOCO-AIDL/pretrained_models/anymal_c/Policy_Reward_6036.56.pt')\n",
        "\n",
        "  action_std = std_init\n",
        "  \n",
        "  for i_video in range(num_videos): \n",
        "    ep_reward = test(action_std, env, policy,i_video)\n",
        "    print(f'Video #{i_video+1} reward: {ep_reward}')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_video(5)"
      ],
      "metadata": {
        "id": "G44DaYd08VBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}